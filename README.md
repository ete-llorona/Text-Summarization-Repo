# Text Summarization Repo
NLP 중에서도 텍스트 요약 관련 다양한 자료를 축적해나가는 공간입니다. 텍스트 요약 분야에 관심 있어 공부를 시작하시는 분들께 좋은 길잡이가 되면 좋겠습니다.

우선은 텍스트 요약 분야의 분류, 주요 연구주제 등 이 분야를 공부하기 전에 알아둬야 할 필수 지식들을 정리했습니다. 그리고 이 분야 흐름을 이해하기 위해 필수적으로 읽어야 할 논문들과, 한국어/영어 요약 모델을 구성하기 위해 이용할 수 있는 Data와 Pre-trained Model, 이들을 이해하기 위해 참고할만한 양질의 자료들을 목록화 했습니다.

  * [Intro to Text Summarization](#intro-to-text-summarization)
      * [Definition](#definition)
      * [Task Categories](#task-categories)
      * [Main Topics](#main-topics)
      * [Prerequisite](#prerequisite)
  * [Resources](#resources)
    + [Must-read Papers](#must-read-papers)
    + [SOTA & Latest Research List](#sota--latest-research-list)
  * [Data & Pre-trained Models](#data--pre-trained-models)
    + [Korean](#korean)
      - [Datasets](#datasets)
      - [Pre-trained Models](#pre-trained-models)
    + [English / Multilingual](#english--multilingual)
      - [Datasets](#datasets-1)
  * [Others](#others)
    + [Services](#services)
    + [Resources](#resources-1)
    + [Recommended Papers list](#recommended-papers-list)

<br>

## Intro to Text Summarization

### Definition

 [Berry, Dumais, & O’Brien (1995)](https://epubs.siam.org/doi/abs/10.1137/1037127)은 텍스트 요약을 다음과 같이 정의합니다.

> Text summarization is the process of **distilling the most important information** from a text to produce an abridged version for a particular task and user

한 마디로 **주어진 텍스트 중 중요한 정보만 정제해내는 과정**이라는 건데요. 여기서 *정제*라는 표현과 *중요한*이라는 표현은 다소 추상적이고 주관적인 표현이기에 개인적으로는 다음과 같이 정의하고 싶습니다.

**`f(text) = comprehensible information`**

즉 텍스트 요약은 **원문을 이해하기 쉬우면서도 가치있는 정보로 변환**하는 작업입니다. 인간은 길이가 길거나 여러 문서로 나눠져있는 텍스트 정보를 한 눈에 파악하기 어려워합니다. 때로는 알지 못하는 전문 용어가 많이 사용되어 있을 수도 있습니다. 이러한 텍스트를 원문을 잘 반영하면서도 간결하여 이해하기 쉬운 형태로 바꿔주는 작업은 상당히 가치있는 일입니다. 물론 무엇이 정말 가치있는 정보인지, 어떠한 형태로 바꿔줄지 등은 요약을 하는 목적이나 개인의 취향에 따라 달라지게 될 것입니다. 

이러한 관점으로 바라본다면 회의록, 신문기사 헤드라인, 논문 abstract, 이력서 등 텍스트를 생성해내는 task뿐만 아니라 텍스트를 그래프나 영상으로 변환하는 작업들도 텍스트 요약에 해당된다고 말할 수 있습니다. 물론 그냥 summarization이 아닌 *Text Summarization*이기에 요약의 대상(source)은 text형태로 한정됩니다. (요약의 source는 text뿐만 아니라 image나 video가 될 수도 있기 때문이죠. 전자의 예가 image captioning, 후자의 예가 [video Summarization](https://paperswithcode.com/task/video-summarization)입니다. 또한 Text, image, video 등 다양한 형태의 source를 함께 요약하는 방식을 multimodal summarization라고 합니다. 물론 Vision과 NLP의 경계가 점점 흐릿해져가는 최근 딥러닝 추세를 고려해본다면, 곧 'Text'를 prefix로 붙이는게 의미 없는일이 되지는 않을까 하는 생각이 들기도 하네요.)

<br>

### Task Categories

일반적으로 텍스트 요약  task는 크게 요약문을 생성하는 방식에 따라 **extractive summarization**(이하 ext)과 **abstractive summarization**(이하 abs)로 나눕니다. ([Gudivada, 2018](https://www.sciencedirect.com/topics/computer-science/text-summarization))

> Extractive methods select a subset of existing words, phrases, or sentences in the original text to form a summary. In contrast, abstractive methods first build an internal semantic representation and then use natural language generation techniques to create a summary. 

**Ext**는 보통 **문장 단위로 중요도를 scoring한 후, 이를 기반으로 선택하고 조합**하여 summary를 만듭니다. 우리가 글을 읽다가 형광펜을 칠하는 작업과 비슷하죠. 반면 **abs**는 **원문을 기반으로 하되, 새로운 텍스트(novel text)를 생성해내는 NLG(natural language generation) 방식**입니다. Ext는 원문에 있던 텍스트를 활용하기에 표현이 제한적이나 말이 안되는 표현이 포함될 가능성이 낮습니다. 반면 abs는 모델에서 새로운 텍스트를 생성해내야 하기에 말이 되지 않는 표현이 만들어질 가능성이 존재하나 좀더 flexible한 접근이 가능하다는 장점이 있습니다.

이 외에도 원문의 개수에 따라 **single/multi document** summarization, 생성해내는 텍스트 형태에 따라 **keyword/sentence** summarization, 요약 과정에서 원문 외 외부 정보를 얼마나 사용하는지에 따라 **knowlege-poor/rich** summarization 등 다양한 구분이 있습니다.

![Figure 2.1: Classification of summarization tasks.](images/Classification_of_summarization_tasks.png)

(G. Sizov(2010). [Extraction-Based Automatic Summarization: Theoretical and Empirical Investigation of Summarization Techniques](https://www.semanticscholar.org/paper/Extraction-Based-Automatic-Summarization%3A-and-of-Sizov/2d27fd9af4b10cc5b54a849a3c2ad84755b3b13c))

<br>

### Main Topics

Text Summarization 분야의 주요 연구주제를 살펴보고 이 분야에 어떤 Challenge가 존재하는지 함께 생각해봅시다.

- **Multi / Long documents summarization**

  앞서 언급했듯 요약이라는 task는 incomprehensible text를 comprehensible information로 바꾸는 작업입니다. 그렇기에 원문이 길어질수록, 또는 한 번에 한 문서가 아닌 여러 소스의 문서를 요약할수록 요약의 효용은 증가합니다. 문제는 동시에 요약 난이도 또한 증가한다는 점이겠죠.

  그 이유로는 첫째, 원문이 길면 길수록 computational complexity가 더 급격하게 증가합니다. 이는 과거의 TextRank 같은 통계 방식에서 보다, 최근 transformer를 위시한 신경망 기반 방식에서 훨씬 더 critical한 문제입니다. 둘째, 원문이 길수록 그 안에 핵심이 아닌 내용, 즉 noise가 많이 포함되어 있기 마련입니다. 무엇이 noise고 무엇이 informative한 텍스트인지 가려내기가 쉽지 않습니다. 마지막으로 긴 원문이나 다양한 소스는 다양한 관점과 내용을 동시에 가지고 있기에 이를 잘 포괄하는 요약문을 생성하는 것이 어려워지죠. 

  - **Multi documents summarization(MDS)**

    MDS는 **복수개의 문서를 요약**하는 작업입니다. 일견 생각해도 하나의 주제를 일관된 흐름과 관점으로 서술하고 있는 하나의 문서를 요약하는 작업보다 다양한 저자들의 서로 다른 관점의 글들을 요약하는 일이 어려울 것입니다. 물론 MDS의 경우에도 보통은 유사한 주제를 다루고 있는 동일 cluster 문서임을 전제로 하고 있지만, 여러 문서 중 어떻게 하면 중요한 정보를 식별하고 중복 정보를 걸러낼 수 있는가는 쉽지 않은 문제입니다. 

    특정 제품에 대한 리뷰를 요약하는 task는 가장 쉽게 생각할 수 있는 MDS의 예입니다. 보통 Opinion summarization라고 불리는 이 task는 텍스트 길이가 짧고 주관성이 높다는 특징을 가집니다. 위키문서를 생성해내는 작업 또한 MDS로 생각해볼 수 있습니다.  [Liu et al. (2018)](https://arxiv.org/abs/1801.10198)는 위키문서에 reference로 달린 웹사이트 본문들을 원문으로, 해당 위키문서 문장들을 summary로 간주하여 데이터화 하고, 이를 통해 위키 생성 모델을 만듭니다.

  - **Long documents summarization** 

    [Liu et al. (2018)](https://arxiv.org/abs/1801.10198)는 긴 텍스트를 인풋으로 받아들이기 위해 우선 통계적 방법으로 extractive summary를 만들어 중요한 문장만 추린 후 모델의 입력으로 사용합니다. 또한 transformer 연산량을 줄이기 위해 input을 블락 단위로 나눠서 연산하고 이 때 1-d convolution을 적용하여 개별 attention key, value의 수를 줄인 attention 방식을 사용합니다. [Big Bird (2020)](https://arxiv.org/abs/2007.14062) 논문은 transformer의 계산량을 줄이기 위해 기존 모든 단어 간 조합을 살펴보는 full attention 방식(quadratic) 대신 sparse attention mechanism(linear)을 도입합니다. 그 결과 동일 성능 하드웨어로 최대 8배까지 긴 문장을 요약할 수 있었습니다. 

    반면 [Gidiotis &Tsoumakas (2020)](https://arxiv.org/abs/2004.06190)은 긴 텍스트 요약 문제를 한 번에 풀지 않고 여러 작은 텍스트 요약 문제들로 바꿔 푸는 divide-and-conquer 접근을 시도합니다. 원문과 target summary를 multiple smaller source-target pairs로 바꿔서 모델을 훈련합니다. inference 시에는 이 모델을 통해 출력된 partial summaries를 aggregate하여 complete summary를 만듭니다. 

  <br>

- **Performance improvement**

  어떻게 하면 더 좋은 요약문을 생성해낼 수 있을까요?

  - **Transfer Learning**

    최근 NLP에서 Pretraining model을 이용하는 것은 거의 default가 되었습니다. 그렇다면 Text Summarization에서 좀 더 좋은 성능을 보여줄 수 있는 Pretraining model을 만들기 위해서는 어떤 구조를 가져야 할까요? 어떤 objective를 가져야 할까요?

    [PEGASUS (2020)](#pegasus)에서는 텍스트 요약과정과 objective가 유사할수록 높은 성능을 보여줄 것이라는 가정하에  ROUGE score에 기반하여 중요하다고 판단되는 문장을 골라 문장 단위로 마스킹하는 GSG(Gap Sentences Generation) 방식을 사용했습니다. 현 SOTA 모델인 [BART (2020)](https://arxiv.org/abs/1910.13461)(Bidirectional and Auto-Regressive Transformers)는 입력 텍스트 일부에 노이즈를 추가하여 이를 다시 원문으로 복구하는 autoencoder 형태로 학습합니다.

  - **Knowledge-enhanced text generation**

    Text-to-text task에서는 원문만으로는 원하는 출력을 생성하는 것이 어려운 경우가 많습니다. 그래서 **원문뿐만 아니라 추가로 다양한 knowledge를 모델에 제공하여 성능을 높이려는 시도**가 있습니다. 이러한 knowledge의 source나 제공 형태는 keywords, topics, linguistic features, knowledge bases, knowledge graphs, grounded text 등 다양합니다.

    예를 들어 [Tan, Qin, Xing, & Hu(2020)](#Any_Aspects)는 일반적인 summry dataset을 복수개의 Aspect-based Summary로 변환하기 위해 knowledge graph인 ConceptNet을, 주어진 aspect와 관련한 더 풍성한 정보를 모델에 전달하기 위해 Wikipedia를 활용합니다. 보다 더 자세히 알고 싶다면 [Yu et al. (2020)](https://arxiv.org/abs/2010.04389)가 쓴 survey 논문을 읽어보세요.

  - **Post-editing Correction**

    한 번에 좋은 요약문을 생성해내면 좋겠지만, 쉽지 않은 일입니다. 그렇다면 우선 요약문을 생성한 후 이를 다양한 기준에서 검토하고 수정해보면 어떨까요?

    일례로 [Cao, Dong, Wu, &Cheung (2020)](https://arxiv.org/abs/2010.08712)은 생성된 요약문에 pretrained neural corrector model을 적용하여 Factual Error를 감소시키는 방법을 제시합니다.

  - 이 외에도 최근 많은 관심을 받고 있는 **Graph Neural Network(GNN)**을 요약에 적용하는 시도도 많아지고 있습니다.

  <br>

- **Data scarcity problem**

  텍스트 요약이란 task는 사람이 하기에도 쉽지 않은, 시간이 많이 소모되는 작업입니다. 따라서 다른 task와 비교해도 labeled dataset을 만드는데 상대적으로 더 큰 비용이 소모되고 당연히 training을 위한 데이터가 많이 부족합니다.

  - 이에 따라 앞서 언급한 pretraining model을 이용하는 **transfer Learning**방식 외에도 **unsupervised learning**이나 **reinforcement learning** 방식으로 학습하거나 **few-Shot Learning**적 접근을 시도하고 있습니다. 

  - 당연히 좋은 요약 데이터를 만드는 것 자체도 굉장히 중요한 연구 주제입니다. 특히나 현재 만들어진 summarization 관련 데이터셋의 상당수가 english 언어로 된 news type에 편중되어 있습니다. 이에 따라 최근 [WikiLingua](#WikiLingua)나 [MLSUM](#mlsum)과 같은 multilingual datasets이 만들어지고 있습니다. 보다 자세한 내용은  [MLSUM: The Multilingual Summarization Corpus](https://www.aclweb.org/anthology/2020.emnlp-main.647.pdf)의 *2. Related Work*를 살펴보세요.

  <br>

- **Metric** / **Evaluation method**

  앞서 '좋은'이라는 두루뭉술한 표현을 썼는데요. 과연 '좋은 요약문'이란 무엇일까요?  [Brazinskas, Lapata, & Titov (2020)](https://arxiv.org/abs/2004.14884)에서는 좋은 요약문의 판단기준으로 다음 다섯가지를 사용합니다.

  - **Fluency**:  the summary should be grammatically correct, easy to read and understand; 
  - **Coherence**: the summary should be well structured and well organized; 
  - **Nonredundancy**: there should be no unnecessary repetition in the summary; 
  - **Informativeness**: how much useful information about the product does the summary provide?
  - **Sentiment**: how well the sentiment of the summary agrees with the overall sentiment of the original reviews?

  문제는 이러한 부분을 측정하는 것이 쉽지 않다는 점입니다. 텍스트 요약에서 가장 흔히 사용되는 성능 측정 지표는 ROUGE score인데요. ROUGE score에는 다양한 변종이 있지만, 기본적으로 'generated summary와 reference summary의 출현 단어와 그 순서가 얼마나 일치하는가'를 측정합니다. 뜻은 유사하지만 형태는 다른 단어가 나오거나, 단어 순서가 달라지면 설혹 더 좋은 요약문일지라도 낮은 점수를 받을 수 있겠죠. 특히 ROUGE score를 높이려다 오히려 요약문의 표현적 다양성(diversity)을 해치는 결과를 가져올 수 있습니다. 이 때문인지 많은 논문에서는 ROUGE score 뿐만 아니라 비싼 돈이 들어가는 human evaluation 결과를 추가로 제시합니다.  

  [Lee et al. (2020)](https://arxiv.org/abs/2005.03510)은 generated summary가 본문 및 reference summary와 얼마나 유사한지를 이들을 SBERT로 임베딩한 후 나오는 벡터 간 유사도로 측정하는 RDASS(Reference and Document Awareness Semantic Score)를 제시합니다. 이러한 방식은 특히 단어와 여러 형태소가 결합하여 다양한 의미와 문법적 기능을 표현하는 교착어인 한국어 평가 정확도를 올려줄 것으로 기대됩니다. [Kryściński, McCann, Xiong, & Socher (2020)](https://arxiv.org/abs/1910.12840)는 Factual Consistency를 평가하기 위한 weakly-supervised, model-based approach를 제안했습니다.

  <br>

- **Controllable text generation**

  주어진 문서에 대해 최고의 요약문은 오직 하나일까요? 그렇지 않을 겁니다. 성향이 다른 사람들은 동일한 원문에 대해서도 서로 다른 요약문을 더 선호할 수 있습니다. 혹 동일한 사람일지라도  요약을 하는 목적이나 상황에 따라 원하는 요약문은 달라지겠죠. 이렇게 **사용자가 지정한 조건에 따라 출력을 원하는 형태로 조정하는 방식**을 *controllable text generation*라고 합니다. 주어진 문서에 대해 동일한 요약문을 생성하는 *Generic summarization*에 비해 **개인화**된 요약문을 제공할 수 있습니다. 

  이렇게 생성되는 요약문은 이해하기 쉽고 가치있어야 할 뿐만 아니라 **함께 넣어준 condition과 밀접하게 연관성을 지녀야 합니다.**

  `f(text, `**`condition`**`) = comprehensible information`**`that meets the given conditions`**

  요약모델에 어떤 *condition*을 추가해볼 수 있을까요? 그리고 어떻게 하면 그 조건에 맞는 요약문을 생성해낼 수 있을까요?

  - **Aspect-based summarization**

    에어팟 사용자 리뷰를 요약할 때 음질, 배터리, 디자인 등의 측면을 나눠 각각에 대한 요약을 하고 싶을 수 있습니다. 아니면 글의 writing style이나 sentiment를 조정해보고 싶을 수도 있습니다. 이렇게 원문에서도 **특정한 측면(aspect) 또는 특성(feature)과 관련있는 정보만을 요약하는 작업**을 *aspect-based summarization*라고 합니다.

    기존에는 주로 모델 학습 시 사용했던 pre-defined aspect에서만 제한적으로 작동하는 모델만 가능했지만 최근들어 [Tan, Qin, Xing, & Hu(2020)](https://arxiv.org/abs/2010.06792)와 같이 학습 시 주어지지 않았던 **arbitrary aspect에 대한 추론**을 가능케 하기 위한 시도들이 이루어지고 있습니다.  

  - **Query focused summarization**(QFS)

    *Condition*이 *query*인 경우, QFS라 부릅니다. **Query는 주로 자연어 형태**를 띠고 있기에 이 다양한 표현들을 어떻게 잘 임베딩하여 원문과 매칭할지가 주요한 과제입니다. 우리가 잘 아는 QA시스템과 상당히 유사하다고 할 수 있습니다. 
    
  - **Update summarization**

    인간은 계속해서 학습하고 성장해나가는 동물입니다. 따라서 특정 정보에 대해 내가 느끼는 오늘의 가치는 일주일 후의 가치와 전혀 다를 수 있습니다. 내가 이미 경험한 문서에 있던 내용의 가치는 낮아질 것이고, 아직 경험하지 못한 새로운 내용은 여전히 높은 가치를 지닐 것입니다. 이러한 관점에서 **해당 이용자가 이전에 경험했던 문서 내용과 유사도가 낮은 새로운 내용 위주의 요약문을 생성하는 작업**을 *update summarization*이라고 합니다.

  <br>

- 이 외에도 **모델 경량화(lightweight deep learning)**와 같은 전형적인 DL 주제는 물론이고 뉴스나 위키백과와 같은 Structured text가 아닌 **대화체 요약(conversation summarization)** 등에 적합한 요약모델을 만들고자 하는 시도 등 다양한 주제들이 있습니다.  

<br>

### Related Knowledge

텍스트 요약 분야 공부를 해나가는데 있어 다음을 알고 있다면 좀 더 쉽게 공부를 진행해나갈 수 있을 것입니다.

#### Required

- NLP 기본 개념 이해

  - Embedding
  - Transfer learning(Pre-training  + Fine-tuning)

- Transformer/BERT 구조 및 Pre-training objective 이해

  최신 NLP 분야 논문들의 상당수가 Transformer에 기반하여 만들어진 BERT, 그리고 이 BERT의 변형인 RoBERTa, T5 등 여러 Pretraining model에 기반하고 있습니다. 따라서 이들의 개략적 구조와  Pre-training objective에 대해 얕게나마 이해하고 있다면 논문을 읽거나 구현함에 있어 큰 도움이 됩니다. 

  - [영상] [이유경(KoreaUniv DSBA) . Transformer to T5 (XLNet, RoBERTa, MASS, BART, MT-DNN,T5)](https://www.youtube.com/watch?v=v7diENO2mEA)
    Transformer에 기반한 최신 모델들을 objective 중심으로 비교하며 설명해줍니다.
  - [영상] [고현웅. Machine Translation Survey (vol1) : Background)](https://www.youtube.com/watch?v=KQfvEg-fGMw)

- Text Summarization 기초 개념

  - Summarization 기본 용어
    - *Original text* = *Source text*
    - *generated summary*는 모델이 생성해낸 요약문을 의미합니다. 반면 우리가 정답으로 간주하는(보통은 사람이 직접 원문을 보고 생성한) 요약문은 *reference summary* 또는 *gold summary*라고 부릅니다. 보통은 두 용어를 크게 구분없이 쓰는듯 하나, 전자는 *generated summary*를 평가하기 위한 기준이 되는 요약문이라는 면을 강조할 때, 후자는 우리가 찾는 진짜 요약문이라는 점을 강조할 때 주로 사용되는 듯 합니다.  
  - Metric: Rouge, BLEU, Perplexity(PPL) 등
  - [글] [icoxfog417. awesome-text-summarization](https://github.com/icoxfog417/awesome-text-summarization)
  - [PPT] [Sang-Houn Choi. Text summarization](https://www.slideshare.net/cozyhous?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview)

#### Elective

- Graph Neural Network(GNN)

  - [영상] [강현규(KoreaUniv DMQA). Graph Attention Networks](http://dmqm.korea.ac.kr/activity/seminar/296)

- Machine translation(MT)

  MT는 seq2seq의 등장 이후 NLP분야 중에서도 가장 활발하게 연구되어온 task 중 하나입니다. Summarization 과정을 원 텍스트를 다른 형태의 텍스트로 변환하는 과정으로 본다면 일종의 MT라고도 볼 수 있기에, MT 관련 연구와 아이디어 중 많은 부분이 summarization 분야에 이미 차용되었거나 앞으로 적용될 가능성이 높습니다. 

  - [영상] [고현웅. Machine Translation Survey (vol2) : Background)](https://www.youtube.com/watch?v=18iH6VX-IU4)

<br>

## Resources

### Must-read Papers

| Year                              | Paper                                                        | Keywords                                                     |
| :---------------------------------: | ------------------------------------------------------------ | :------------------------------------------------------------: |
| 2004<br /><br />Model             | [**TextRank**: Bringing order into texts](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)<br />*R. Mihalcea, P. Tarau*<br /><br />추출요약 분야의 고전이자 지금도 활발하게 사용되고 있는 대표적인 요약모델입니다. 문서 내에서 중요한 sentence(즉 summary에 포함될만한)는 다른 sentence들과 높은 similarity를 가지고 있을 것이라는 가정 하에 구글 검색엔진의 초기 아이디어인 PageRank 알고리즘을 차용하는데요. 각 sentence가 문서 내 다른 sentence와 가지는 similarity를 recursive하게 계산하기 위해 sentence-level weighted graph를 구성하고, 이 weight가 높은 sentence를 summary에 포함합니다. <br />통계 기반 unsupervised learning 방식으로 별도의 학습 데이터 없이 추론이 가능하고, 알고리즘이 명확하여 이해가 쉽습니다. <br /><br />- [Library] [gensim.summarization](https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_summarization.html#sphx-glr-auto-examples-tutorials-run-summarization-py)(3.x버전만 가능. 4.x버전에서 삭제),  [PyTextRank](https://github.com/DerwenAI/pytextrank)<br />- [이론/Code] [lovit. TextRank 를 이용한 키워드 추출과 핵심 문장 추출](https://lovit.github.io/nlp/2019/04/30/textrank/) | ext,<br />Graph-based(PageRank),<br />Unsupervised           |
| 2019<br /><br />Model             | **BertSum**: [Text Summarization with Pretrained Encoders](https://arxiv.org/pdf/1908.08345.pdf) ([Code](https://github.com/nlpyang/PreSumm))<br/>*Yang Liu, Mirella Lapata / EMNLP 2019*<br /><br />![BERTSUM_structure](images/BERTSUM_structure.PNG) Pre-trained BERT를 요약 task에 활용하려면 어떻게 해야할까요? <br />BertSum은 여러 sentence를 하나의 인풋으로 넣어주기 위해 매 문장 앞에 [CLS] 토큰을 삽입하고 interval segment embeddings을 추가한 변형 input embeddings을 제안합니다. <br />ext 모델은 BERT에 Transformer layers를 얹은 encoder 구조를, abs 모델은 ext 모델 위에 6-layer Transformer decoder를 얹은 encoder-decoder 모델을 사용합니다.<br /><br />- [Review] [이정훈(KoreaUniv DSBA)](https://www.youtube.com/watch?v=PQk9kr9dGu0)<br />- [Code] [KoBertSum(수정중)](https://github.com/uoneway/KoBertSum) | ext/abs, <br />BERT+transformer,<br />2-staged fine-tuning   |
| 2019<br /><br />Pretraining Model | <a name="BART"></a>[**BART**: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://www.aclweb.org/anthology/2020.acl-main.703/) <br />*Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer / ACL 2020*<br /><br />![img](images/summarization.016.png) BERT는 bidirectional encoder로 generation task에 약하고, GPT는 auto-regressive한 model로 bidirectional한 정보를 얻지 못한다는 단점을 가집니다. <br />BART는 이들을 결합한 seq2seq 형태를 가짐으로 기존에 나왔던 다양한 denosing기법을 한 모델에서 실험해볼 수 있었습니다. 그 결과 Text infilling(text span을 하나의 mask token으로 바꿈)과 Sentence shuffling(문장 순서를 랜덤으로 섞음)을 동시에 적용함으로, 특히 summarization 분야에서 기 SOTA 모델을 훌쩍 뛰어넘는 성능을 보여줍니다.  <br /> <br />- [Code] SKT T3K. **[KoBART](https://github.com/SKT-AI/KoBART)**<br />- [Review] [진명훈_영상](https://www.youtube.com/watch?v=VmYMnpDLPEo), [임연수_글](https://dladustn95.github.io/nlp/BART_paper_review/),  [Jiwung Hyun_글](https://medium.com/@kabbi159/acl-2020-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-7a0ae37109dc), | abs,<br />seq2seq,<br />Denoising autoencoder,<br />Text infilling |
| 2020<br /><br />Model             | [**MatchSum**: Extractive Summarization as Text Matching](https://arxiv.org/abs/2004.08795) ([Code](https://github.com/maszhongming/MatchSum))<br />*Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang / ACL 2020*<br /><br />- [Review] [이유경(KoreaUniv DSBA)](https://www.youtube.com/watch?v=8E2Ia4Viu94&t=1582s) | ext                                                          |
| 2020<br /><br />Technique         | <a name="Any_Aspects"></a>[Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach](https://arxiv.org/abs/2010.06792) ([Code](https://github.com/tanyuqian/aspect-based-summarization))<br />*Bowen Tan, Lianhui Qin, Eric P. Xing, Zhiting Hu / EMNLP 2020*<br /><br />![image-20210109063052942](images/image-20210109063052942.png) <br />Aspect-based summarization는 1) multiple aspect-based summaries data가 부족하고, 2) 모델을 학습한다 해도 학습한 data의 pre-defined aspects에서만 제한적으로 작동한다는 점에서 쉽지 않은 task입니다. <br />본 논문은 이 문제를 해결하기 위해 external knowledge sources를 활용합니다. <br />- generic summary를 multiple aspect-based summaries로 변환하기 위해 크게 두 단계를 거칩니다. 우선 aspect 수를 늘려주기 위해 generic summary에서 추출한 entity를 seed삼아 ConcepNet에서 그 이웃 entities까지 추출하고 이들 각각을 하나의 aspect로 간주합니다. 이 각각의 aspect에 대한 psedo summary를 만들어내기 위해 다시 ConcepNet을 이용하는데요. ConcepNet에서 해당 aspect와 연결된 주위 entity를 추출하고, generic summary 내에서 이들이 포함되어 있는 문장만 추출하여 concat합니다. 이를 해당 entity(aspect)에 대한 summary로 간주합니다.<br />-  주어진 aspect와 관련한 더 풍성한 정보를 모델에 전달하기 위해 Wikipedia를 활용합니다. 구체적으로는 해당 문서 내에서 등장하는 단어 중, 문서 내 TF-IDF 점수가 높으면서 동시에 해당 aspect에 해당하는 Wikipedia 페이지에 등장하는 10개 이하의 단어목록을 모델 input으로 aspect와 함께 넣어줍니다. <br />이러한 방식으로 pretraining model(BART)을 fine-tuning함으로, 적은 데이터로 arbitrary aspect에 대해서도  우수한 성능을 보였습니다. | Aspect-based,<br />Knowlege-rich                             |
| 2020<br /><br />Review            | [**What Have We Achieved on Text Summarization?**](https://arxiv.org/abs/2010.04529)<br />*Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, Yue Zhang / EMNLP 2020*  <br /><br />ROUGE score뿐만 아니라 Accuracy 및 Fluency 관련 8 metrics(PolyTope)에 따라 10개의 대표적인 요약모델을 평가합니다. 그 결과를 요약해보자면, <br />- 전통적 rule-based 방법은 여전히 baseline으로 유효하다.<br />- 유사한 세팅 하에서는 abs모델보다 **ext모델**이 대체적으로 faithfulness와 factual-consistency에서 더 좋은 성능을 보임. The main shortcoming is unnecessity for extractive models, and omission and intrinsic hallucination for abstractive models.<br />- Sentence representation을 생성하기 위한 **transformer와 같은 더 복잡한 구조**가 duplication 문제 말고는 크게 도움이 안됨<br />- Copy(**Pointer-Generator**)는 reproducing detail하는데, Inaccuracy Intrinsic 뿐만 아니라 추출/생성을 적절히 혼합하여 word level duplication 문제도 효과적으로 해결함. but tends to cause redundancy to a certain degree.<br /> **Coverage**는 by a large margin으로repetition errors(Duplication)은 감소시키나, 동시에 Addition과 Inaccuracy Intrinsic error를 증가시키는 한계 <br />- ext후 abs하는 **hybrid 모델**은 원문의 일부(extracted snippets)을 통해 요약을 생성해내기에 recall은 좋지만 Inaccuracy error에 문제가 있을 수 있음<br />- Pre-training, 특히 encoder only 모델(BertSumExtAbs)보다 encoder-decoder 모델(BART)은 요약분야에서 아주 효과적임. 이는 입력에 대한 이해 및 생성을 모두 pretraining하는 것이 content selection과 combination에 아주 유용함을 시사함. 동시에 대부분의 abs 모델이 앞쪽 문장만 집중하는데 반해 BART가 원문 전반을 모두 보고 있다는 점은 pretraing 시 sentence shuffling의 효과로 보임<br />![image-20210109062449344](images/image-20210109062449344.png)<br /><br />- [Review] [김한길](https://www.slideshare.net/hangilkim75/paper-review-what-have-we-achieved-on-text-summarization), [허훈](https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/summarization_achievement.md) |                                                              |


<br>

### SOTA & Latest Research List

#### SOTA Model List

- [Papers with Code: Best method for each benchmarks](https://paperswithcode.com/task/text-summarization)
- [NLP-progress](https://github.com/sebastianruder/NLP-progress)

#### Latest Research List

- [Paper Digest: Recent Papers on Text Summarization](https://www.paperdigest.org/2020/08/recent-papers-on-text-summarization/)

- [Papers with Code: Latest papers](https://paperswithcode.com/task/text-summarization/latest#code)

- [EMNLP 2020 Papers-Summarization](https://github.com/roomylee/nlp-papers-with-arxiv/tree/master/emnlp-2020#summarization)

<br>

## Data & Pre-trained Models

아래 사용한 약자의 의미는 다음과 같습니다.

* `w`: The number of words의 평균값;  `s`: The average number of sentences의 평균값

  예) `13s/214w → 1s/26w` 는 평균 13문장(평균 214단어)으로 구성된 본문 텍스트와 평균 1개 문장(평균 26개 단어)로 구성된 요약 텍스트가 제공된다는 의미입니다.

* `abs`: Abstractive summary; `ext`: Extractive summary

<br>

### Korean

#### Datasets

| Dataset                                                      | Domain / Length                                    | Volume<br />(pair)                | License                                                      |
| ------------------------------------------------------------ | -------------------------------------------------- | --------------------------------- | ------------------------------------------------------------ |
| [**모두의 말뭉치-문서 요약 말뭉치**](https://corpus.korean.go.kr/) ([명세](https://rlkujwkk7.toastcdn.net/NIKL_SUMMARIZATION(v1.0).pdf))<br />짧은 뉴스 본문에 대한 제목, 3문장 abs 및 ext summay 제공. id로 신문 말뭉치([명세](https://rlkujwkk7.toastcdn.net/NIKL_NEWSPAPER(v1.0).pdf))와 결합하여 소제목, media, date, topic 관련 추가 정보를 얻을 수 있음 | 뉴스<br />\- origin → 3s(abs); 3s(ext)             | 13,167                            | 국립국어원<br />(개별 약정)                                  |
| [**sae4K**](https://github.com/warnikchow/sae4k)             |                                                    | 50,000                            | [CC-BY-SA-4.0](https://github.com/warnikchow/sae4k/blob/master/LICENSE) |
| **[sci-news-sum-kr-50](https://github.com/theeluwin/sci-news-sum-kr-50)** | 뉴스(IT/과학)<br />                                | 50                                | MIT                                                          |
| **[Bflysoft 구축-뉴스기사 데이터셋 (2020)](https://dacon.io/competitions/official/235671/data/)**<br />짧은 뉴스 본문에 대한 media, 단문 abs 및 ext summay 제공<br /><br />- [EDA] [데이터 EDA 노트북](https://github.com/uoneway/KoBertSum/blob/master/tutorials/EDA.ipynb)<br />- [한국어 문서 추출요약 AI 경진대회(~ 2020.12.09)](https://dacon.io/competitions/official/235671/overview/)<br />- [한국어 문서 생성요약 AI 경진대회(~ 2020.12.09)](https://dacon.io/competitions/official/235673/overview/) | - 뉴스<br />\- 13s/214w → 1s/26w(abs); 3s/55w(ext) | 43,000                            | 개별 약정<br />* 추후 [AIHub](https://www.aihub.or.kr/aidata/8054)를 통해 공개예정 |
| <a name="WikiLingua">[**WikiLingua**: A Multilingual Abstractive Summarization Dataset (2020)](https://github.com/esdurmus/Wikilingua)<br />매뉴얼 사이트인 [WikiHow](https://www.wikihow.com/)를 기반으로 Korean, English 등 18개국어 summay dataset 제공<br /><br />- [paper](https://arxiv.org/abs/2010.03093), [Collab notebook](https://colab.research.google.com/drive/1HxonmcM7EOQVal2I6oTi9QWEP257BgDP?usp=sharing)</a> | - How-to docs<br />- 391w→ 39w                     | 12,189<br />(전체 770,087 중 kor) | 2020,<br />[CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/) |

<br>

#### Pre-trained Models

| Model                                                        | Pre-training                                                 | Usage                                                        | License                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------- |
| [**BERT(multilingual)**](https://github.com/google-research/bert/blob/master/multilingual.md)<br />BERT-Base(110M parameters) | - Wikipedia(multilingual)<br />- WordPiece. <br />- 110k shared vocabs | - [`BERT-Base, Multilingual Cased`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) 버전 권장<br />(`--do_lower_case=false` 옵션 넣어주기)<br />- Tensorflow | Google<br />(Apache 2.0)                     |
| [**KOBERT**](https://github.com/SKTBrain/KoBERT)<br />BERT-Base(92M parameters) | - 위키백과(문장 5M개), 뉴스(문장 20M개)<br />- [SentencePiece](https://github.com/google/sentencepiece)<br />- 8,002 vocabs(unused token 없음) | - PyTorch<br />- [KoBERT-Transformers(monologg)](https://github.com/monologg/KoBERT-Transformers)를 통해 Huggingface Transformers 라이브러리로 이용 가능, [DistilKoBERT](https://github.com/monologg/DistilKoBERT) 이용 가능 | SKTBrain<br />(Apache-2.0)                   |
| [**KorBERT**](https://aiopen.etri.re.kr/service_dataset.php)<br />BERT-Base | - 뉴스(10년 치), 위키백과 등 23GB<br />- [ETRI 형태소분석 API](https://aiopen.etri.re.kr/service_api.php) / WordPiece(두 버전을 별도로 제공)<br />- 30,349 vocabs<br />- Latin alphabets: Cased<br />- [소개] [임준(ETRI). NLU Tech Talk with KorBERT](https://www2.slideshare.net/LGCNSairesearch/nlu-tech-talk-with-korbert) | - PyTorch, Tensorflow <br/>                                  | ETRI<br />(개별 약정)                        |
| **[KcBERT](https://github.com/Beomi/KcBERT)**<br />BERT-Base/Large | - 네이버 뉴스 댓글(12.5GB, 8.9천만개 문장)<br />(19.01.01 ~ 20.06.15 기사 중 댓글 많은 기사 내 댓글과 대댓글)<br />- [tokenizers](https://github.com/huggingface/tokenizers)의 BertWordPieceTokenizer<br />- 30,000 vocabs |                                                              | [Beomi](https://github.com/Beomi)<br />(MIT) |
| **[KoBART](https://github.com/SKT-AI/KoBART)**<br />[BART](#BART)(124M) | - 위키백과(5M), 기타(뉴스, 책, 모두의 말뭉치 (대화, 뉴스, ...), 청와대 국민청원 등 0.27B)<br />- [tokenizers](https://github.com/huggingface/tokenizers)의 Character BPE tokenizer<br />- 30,000 vocabs(<unused> 포함)<br /><br />- [Example] seujung. KoBART-summarization([Code](https://github.com/seujung/KoBART-summarization), [Demo](http://20.194.43.11:7874/)) | - 요약 task에 특화<br />- Huggingface Transformers 라이브러리 지원<br />- PyTorch | SKT *T3K*<br />(modified MIT)                |

- 기타
  - https://github.com/snunlp/KR-BERT
  - https://github.com/tbai2019/HanBert-54k-N

<br>

### English / Multilingual

#### Datasets

[기타 요약 관련 영어 데이터셋 명칭, domain, task, paper 등](http://pfliu.com/pl-summarization/summ_data.html)

| Dataset                                                      | Domain / Length                                              |         Volume         |                           License                            |
| ------------------------------------------------------------ | ------------------------------------------------------------ | :--------------------: | :----------------------------------------------------------: |
| **[ScisummNet](https://cs.stanford.edu/~myasu/projects/scisumm_net/)**([paper](https://arxiv.org/abs/1909.01716))<br />ACL(computational linguistics, NLP) research papers에 대한 세 가지 유형의 summary(논문 abstract, collection of citation sentences, human summary) 제공<br /><br />- CL-SciSumm 2019-Task2([repo](https://github.com/WING-NUS/scisumm-corpus), [paper](https://arxiv.org/abs/1907.09854))<br />- [CL-SciSumm @ EMNLP 2020-Task2](https://ornlcda.github.io/SDProc/sharedtasks.html#clscisumm)([repo](https://github.com/WING-NUS/scisumm-corpus)) | - Research paper<br />(computational linguistics, NLP)<br />- 4,417w → 110w(논문abstract) ; 2s(citation); 151w(abs) |    1,000(abs/ ext)     | [CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/legalcode) |
| **[LongSumm](https://github.com/guyfe/LongSumm)**<br />NLP 및 ML 분야 Research paper에 대한 상대적으로 장문의 summary(관련 blog posts 기반 abs, 관련 conferences videos talks 기반 ext) 제공<br /><br /><br />- [LongSumm 2020@EMNLP 2020](https://ornlcda.github.io/SDProc/sharedtasks.html#longsumm)<br />- [LongSumm 2021@ NAACL 2021](https://sdproc.org/2021/sharedtasks.html#longsumm) | - Research paper(NLP, ML)<br />- origin → 100s/1,500w(abs); 30s/ 990w(ext) | 700(abs) +  1,705(ext) | [Attribution-NonCommercial-ShareAlike 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) |
| **[CL-LaySumm](https://github.com/WING-NUS/scisumm-corpus/blob/master/README_Laysumm.md)**<br />NLP 및 ML 분야 Research paper에 대해 비전문가를 위한 쉬운(lay) summary 제공<br /><br />- [CL-LaySumm @ EMNLP 2020](https://ornlcda.github.io/SDProc/sharedtasks.html#laysumm) | - Research paper(epilepsy, archeology, materials engineering)<br />- origin → 70~100w |        600(abs)        | 개별약정 필요([a.dewaard@elsevier.com](mailto:a.dewaard@elsevier.com) 로 이메일을 송부) |
| [**Global Voices**: Crossing Borders in Automatic News Summarization (2019)](http://opus.nlpl.eu/GlobalVoices.php)<br /><br />- [Paper](https://www.aclweb.org/anthology/D19-5411.pdf) | - 뉴스<br />- 359w→ 51w                                      |                        |                                                              |
| <a name="mlsum"></a>[**MLSUM**: The Multilingual Summarization Corpus](https://github.com/recitalAI/MLSUM)<br />CNN/Daily Mail dataset과 유사하게 news articles 내 highlights/description을 summary로 간주하여 English, French, German, Spanish, Russian,Turkish에 대한 summary dataset을 구축 <br /><br />- [Paper](https://www.aclweb.org/anthology/2020.emnlp-main.647), [이용(huggingface)](https://github.com/huggingface/datasets/tree/master/datasets/mlsum) | - 뉴스<br />- 790w→ 56w<br />(en 기준)                       |       1.5M(abs)        |            non-commercial research purposes only             |
|                                                              |                                                              |                        |                                                              |

<br>

## Others

### Services

- [Semantic Scholar](https://tldr.semanticscholar.org/): 논문에 대한 요약서비스 제공
- [TLDR this](https://tldrthis.com/): Article summarizer. Chtome/Firefox extension 제공 
- [세줄요약기](https://summariz3.herokuapp.com/): TextRank 기반 세줄 요약기

<br>

### Recommended Resources

- [KoreaUniv DSBA](https://www.youtube.com/channel/UCPq01cgCcEwhXl7BvcwIQyg/playlists)
- [KoreaUniv DMQA](http://dmqm.korea.ac.kr/activity/seminar)
- [neulab/Text-Summarization-Papers](https://github.com/neulab/Text-Summarization-Papers)
  - [Modern History for Text Summarization](http://pfliu.com/Historiography/summarization/summ-eng.html)
- [mathsyouth/awesome-text-summarization](https://github.com/mathsyouth/awesome-text-summarization)

<br>

### Recommended Papers list

#### Review

| Year | Paper                                                        |
| ---- | ------------------------------------------------------------ |
| 2018 | [A Survey on Neural Network-Based Summarization Methods](https://arxiv.org/abs/1804.04589)<br />Y. Dong |
| 2020 | [Review of Automatic Text Summarization Techniques & Methods](https://pdf.sciencedirectassets.com/280416/AIP/1-s2.0-S1319157820303712/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAMaCXVzLWVhc3QtMSJIMEYCIQCqwas9C5XBrxGWAixtSVG1JHu4Ir1gH4OFpMeFjVcnxQIhAJnmwsesWxU2kSicjrm72Lw1TzC0I1PTDcwulAxemPzhKr0DCIv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1IgxjvutakJbTaBAnOb8qkQMCi48gc%2BweQ6ZCJWzaYTB4ucZyar0sFaZcnzb1wkymdAJwh9m1e%2BwLkZw2xJXLvFlUGn27lEgdYDbka8f%2BohT9oOOkF9QyGIen0yWhqlt4BB0jR6q2PyxdCswlFvY6VBuoK0g9%2Fm6oquTm37MbVHkqnaz70F%2Fy9xn5XpgjPRqrijfCP7Qf8Yd83kfWA7AQ3oxpXwIz8THWSwzlENkVBf8DByWAOvBnBnBD9K1keKjH%2FLQrCSkOgGuNOgaMPm%2FOiCzhRba4bYJJhZChjMcmNqxXczL8ebiCoIydZ923gygB5xDJpqEtP0vt0PpzEa6%2BKi03JJeXQDx3c0qQFejh52UkkqPps9jwF7dGejjgiR8WqNGWrJijW74u%2Bcys2y%2Fv8hcyME4mqlAfiXRPy0qyf6U3NA5EsaFSDR0DXR3bW39F8sIIRCeWOITf6q7rjExzvMdtr%2FsDdtKgghwR9PM75SyvX8FzYeCptHuoR3rfhc3RIxP96MNDdRIbGsht%2BJFuGUYYzuCwXPfUg%2B9eVRuUNT2bSzCPrpj%2BBTrqAep6mCVgTebUDbYKr1eHAVyOOzbsfz4lrlQN4jl3SyAFE%2FYMYxP0AyDB0rIRG8GjzfGKFzqQQScQ77d8m1ECTlFG2IuRqhvuWqIkYt21%2B3OJLSbFJ1kxhR8GLgi1%2BLYU2PJJQoVkhVbzeiPpAYh4vrjx3BdD1Y9xcGRkp5VP01DdkoYlbXpM4OkfTk6las12N8uZIfbSSqnfoepQO%2FunMSudM7nGOVphQU4TsRYDPtVYug1vy8mtj54GhcawwlcsaDPhF2tZ7hdEPyY%2BGSjyXU0ZMTffxJIhPMZUFFEtxjbmzRpSg3%2FEkKyQXQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201201T105450Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY72IXVKMF%2F20201201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=5f657b7900fbfc13936b686de2c66279a3ff74fbf1c0345191c2f0f68223e464&hash=3a55b9be508107240e832695ad5bfc371f18cc0dc0dcef5b45b1067da37346d6&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1319157820303712&tid=spdf-d04b0e55-eee7-401f-836b-11e1fc061edf&sid=2ae121a980d1464ffe6b55c8786454c8f4aagxrqa&type=client)<br />Widyassari, A. P., Rustad, S., Shidik, G. F., Noersasongko, E., Syukur, A., & Affandy, A. |
| 2020 | [A Survey of Knowledge-Enhanced Text Generation](https://arxiv.org/abs/2010.04389)<br />Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang |

<br>

#### Classic

| Year | Paper                                                        | Keywords |
| ---- | ------------------------------------------------------------ | -------- |
| 1958 | [Automatic creation of literature abstracts](http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf)<br />P.H. Luhn | gen-ext  |
| 2000 | [Headline Generation Based on Statistical Translation](http://www.anthology.aclweb.org/P/P00/P00-1041.pdf)<br />M. Banko, V. O. Mittal, and M. J. Witbrock | gen-abs  |
| 2004 | [**LexRank**: graph-based lexical centrality as salience in text summarization](https://www.aaai.org/Papers/JAIR/Vol22/JAIR-2214.pdf)<br />G. Erkan, and D. R. Radev, | gen-ext  |
| 2005 | [Sentence Extraction Based Single Document Summarization](http://oldwww.iiit.ac.in/cgi-bin/techreports/display_detail.cgi?id=IIIT/TR/2008/97)<br />J. Jagadeesh, P. Pingali, and V. Varma | gen-ext  |
| 2010 | [Title generation with quasi-synchronous grammar](https://www.aclweb.org/anthology/D/D10/D10-1050.pdf)<br />K. Woodsend, Y. Feng, and M. Lapata, | gen-ext  |
| 2011 | [Text summarization using Latent Semantic Analysis](https://www.researchgate.net/publication/220195824_Text_summarization_using_Latent_Semantic_Analysis)<br />M. G. Ozsoy, F. N. Alpaslan, and I. Cicekli | gen-ext  |

<br>

#### Based on Neural Net

| Year                                  | Paper                                                        | Keywords                                             |
| ------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------- |
| 2014                                  | [On using very large target vocabulary for neural machine translation](http://www.aclweb.org/anthology/P15-1001)<br />S. Jean, K. Cho, R. Memisevic, and Yoshua Bengio | gen-abs                                              |
| 2015<br /><br />Model                 | **NAMAS**: [A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/abs/1509.00685) ([Code](https://github.com/facebookarchive/NAMAS))<br />A. M. Rush, S. Chopra, and J. Weston / EMNLP 2015<br /><br />기존의 문장 선택 및 조합 방식을 넘어서기 위해 기 Seq2Seq에  target-to-source attention을 도입하여 abstractive summary를 생성합니다. | abs<br />Seq2Seq with att                            |
| 2015                                  | [Toward Abstractive Summarization Using Semantic Representations](https://arxiv.org/pdf/1805.10399.pdf)<br/>Fei Liu,Jeffrey Flanigan,Sam Thomson,Norman M. Sadeh,Noah A. Smith / NAA-CL | abs, <br />task-event,<br /> arch-graph              |
| 2016                                  | [Neural Summarization by Extracting Sentences and Words](https://arxiv.org/pdf/1603.07252.pdf)<br/>Jianpeng Cheng,Mirella Lapata / ACL | gen-2stage                                           |
| 2016                                  | [Abstractive sentence summarization with attentive recurrent neural networks](http://www.aclweb.org/anthology/N16-1012)<br />S. Chopra, M. Auli, and A. M. Rush / NAA-CL | gen-abs,<br />RNN,CNN,<br /> arch-att                |
| 2016                                  | [Abstractive text summarization using sequence-to-sequence RNNs and beyond](https://arxiv.org/abs/1602.06023)<br />R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang / CoNLL | gen-abs, data-new                                    |
| 2017<br /><br />Model                 | [**SummaRuNNer**: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents](https://arxiv.org/abs/1611.04230)<br />R. Nallapati, F. Zhai and B. Zhou<br /><br />![img](images/summarization.005.png) <br />Hierarchical BiGRU 구조로 문서를 인코딩합니다. 우선 sentence별로 첫 번째 BiGRU에 word 단위로 입력하여 sentence vector를 만들고, 이를 다시 BiGRU에 통과시켜 문장별 임베딩(hidden states)을 생성합니다. 이 개별 문장 임베딩과 이들을 wighted sum한 doc vectort를  logistic classifier에 입력해 해당 문장이 요약에 포함시킬지 여부를 판단합니다. | ext,<br />RNN<br />(hierarchical BiGRU)              |
| 2017<br /><br />Model,<br />Technique | [**Pointer-generator**: Get to the point: Summarization with pointergenerator networks](https://arxiv.org/abs/1704.04368) ([Code](https://github.com/abisee/pointer-generator))<br />A. See, P. J. Liu, and C. D. Manning / ACL 2017<br /><br />![img](images/summarization.011.png)<br /> Generator를 통해 vocabulary distrubution을 생성하고, pointer를 통해 원문의 어떤 단어를 copy할 지를 나타내는 attention distribution을 생성한 후, 학습된 생성 확률(Pgen)에 따라 weighted-sum하여 최종 단어 생성분포를 결정합니다. Abstactive 방식인 generator와 extractive 방식인 attention을 결합한 hybrid 방식으로 기 Seq2Seq with Attention 방식이 factual consistency가 낮던 문제를 보완합니다. 또한 특정 단어가 반복해서 생성되는 문제를 해결하고자, 현재까지 사용된 단어별 누적 attention distribution값(coverage vector c)에 기반한 repetition penalty term을 loss에 포함합니다. <br /><br />- [Review] [김형석(KoreaUniv DSBA)](https://brunch.co.kr/@kakao-it/139) | ext/abs,<br />Pointer-Generator, <br />Coverage loss |
| 2017                                  | [A deep reinforced model for abstractive summarization](https://arxiv.org/abs/1705.04304)<br />R. Paulus, C. Xiong, and R. Socher | gen-ext/abs                                          |
| 2017                                  | [Abstractive Document Summarization with a Graph-Based Attentional Neural Model](https://pdfs.semanticscholar.org/c624/c38e53f321a6df2d16bd707499ce744ca114.pdf)<br/>Jiwei Tan,Xiaojun Wan,Jianguo Xiao / ACL | ext, abs,<br />arch-graph, arch-att                  |
| 2017                                  | [Deep Recurrent Generative Decoder for Abstractive Text Summarization](https://arxiv.org/pdf/1708.00625.pdf)<br/>Piji Li,Wai Lam,Lidong Bing,Zihao W. Wang / EMNLP | latent-vae                                           |
| 2017                                  | [Generative Adversarial Network for Abstractive Text Summarization](https://arxiv.org/abs/1711.09357) |                                                      |
| 2018                                  | [Controlling Decoding for More Abstractive Summaries with Copy-Based Networks](https://arxiv.org/abs/1803.07038)<br />N. Weber, L. Shekhar, N. Balasubramanian, and K. Cho | ext/abs                                              |
| 2018<br /><br />Model                 | [**Generating Wikipedia by Summarizing Long Sequences**](https://arxiv.org/abs/1801.10198)<br />P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer / ICLR | ext/abs                                              |
| 2018                                  | [Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models](https://arxiv.org/abs/1801.07704)<br />T. Baumel, M. Eyal, and M. Elhadad | ext/abs                                              |
| 2018<br /><br />Model                 | [Bottom-Up Abstractive Summarization](https://arxiv.org/pdf/1808.10792.pdf)<br/>Sebastian Gehrmann,Yuntian Deng,Alexander M. Rush / EMNLP 2018<br /><br />요약에 사용될만한 단어들을 먼저 추출(ext)한 후, 이를 기반으로 요약을 생성(abs)하는 대표적인 2staged 모델입니다.* | abs, <br />hybrid,<br />bottom-up attention          |
| 2018                                  | [Deep Communicating Agents for Abstractive Summarization](https://arxiv.org/pdf/1803.10357.pdf)<br/>Asli Çelikyilmaz,Antoine Bosselut,Xiaodong He,Yejin Choi / **NAA-CL | abs, task-longtext, arch-graph                       |
| 2018                                  | [Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting](https://arxiv.org/abs/1805.11080)<br />Y. Chen, M. Bansal | gen-ext/abs<br />arch-graph                          |
| 2018                                  | [Ranking Sentences for Extractive Summarization with Reinforcement Learning](https://arxiv.org/pdf/1802.08636.pdf)<br/>Shashi Narayan,Shay B. Cohen,Mirella Lapata | ext, abs, <br />RNN,CNN,<br /> nondif-reinforce      |
| 2018                                  | [BanditSum: Extractive Summarization as a Contextual Bandit](https://arxiv.org/pdf/1809.09672.pdf)<br/>Yue Dong,Yikang Shen,Eric Crawford,Herke van Hoof,Jackie Chi Kit Cheung | ext, abs, <br />RNN,<br /> nondif-reinforce          |
| 2018                                  | [Content Selection in Deep Learning Models of Summarization](https://arxiv.org/pdf/1810.12343.pdf)<br/>Chris Kedzie,Kathleen McKeown,Hal Daumé | ext, <br />task-knowledge                            |
| 2018                                  | [Faithful to the Original: Fact Aware Neural Abstractive Summarization](https://arxiv.org/pdf/1711.04434.pdf) |                                                      |
| 2018                                  | [A reinforced topic-aware convolutional sequence-to-sequence model for abstractive text summarization](https://www.ijcai.org/proceedings/2018/0619.pdf) |                                                      |
| 2018                                  | [Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization]() |                                                      |
| 2018                                  | [Global Encoding for Abstractive Summarization](https://arxiv.org/pdf/1805.03989.pdf) |                                                      |
| 2018                                  | [Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting](https://www.aclweb.org/anthology/P18-1063) |                                                      |
| 2018                                  | [Neural Document Summarization by Jointly Learning to Score and Select Sentences](https://www.aclweb.org/anthology/P18-1061) |                                                      |
| 2018                                  | [Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization](https://aclweb.org/anthology/P18-1015) |                                                      |
| 2019<br /><br />Model                 | [Fine-tune BERT for Extractive Summarization](https://arxiv.org/abs/1903.10318)<br />Y. Liu | gen-ext<br />                                        |
| 2019                                  | [Pretraining-Based Natural Language Generation for Text Summarization](https://arxiv.org/abs/1902.09243)<br />H. Zhang, J. Xu and J. Wang | gen-abs                                              |
| 2019                                  | [Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization](https://arxiv.org/pdf/1906.00072.pdf)<br/>Sangwoo Cho,Logan Lebanoff,Hassan Foroosh,Fei Liu / ACL | task-multiDoc                                        |
| 2019                                  | [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf)<br/>Xingxing Zhang,Furu Wei,Ming Zhou / ACL | arch-transformer                                     |
| 2019                                  | [ Searching for Effective Neural Extractive Summarization: What Works and What's Next](https://arxiv.org/pdf/1907.03491.pdf) Ming Zhong,Pengfei Liu,Danqing Wang,Xipeng Qiu,Xuanjing Huang / ACL | gen-ext                                              |
| 2019                                  | [BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle](https://arxiv.org/pdf/1909.07405.pdf)<br/>Peter West,Ari Holtzman,Jan Buys,Yejin Choi / EMNLP | gen-ext, sup-sup, sup-unsup, arch-transformer        |
| 2019                                  | [Scoring Sentence Singletons and Pairs for Abstractive Summarization](https://arxiv.org/pdf/1906.00077.pdf)<br/>Logan Lebanoff,Kaiqiang Song,Franck Dernoncourt,Doo Soon Kim,Seokhwan Kim,Walter Chang,Fei Liu | gen-abs, arch-cnn                                    |
| 2019<br /><br />Model                 | <a name="pegasus"></a>[**PEGASUS**: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) ([Code](https://github.com/google-research/pegasus))<br />Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu / ICML 2020<br /><br />![image-20210109064954956](images/image-20210109064954956.png) <br />PEGASUS는 Pretraining objective가 텍스트 요약 과정과 흡사할수록 높은 성능을 보여줄 것이라는 가정하에 ROUGE score에 기반하여 중요하다고 판단되는 문장을 골라 문장 단위로 마스킹하는 GSG(Gap Sentences Generation) 방식을 사용했습니다.<br /><br />- [Review] 김한길. [영상](https://www.youtube.com/watch?v=JhGmeQBbDdA), [발표자료](https://www2.slideshare.net/hangilkim75/pegasus-237175343) |                                                      |
| 2020<br /><br />Model                 | [TLDR: Extreme Summarization of Scientific Documents](https://arxiv.org/abs/2004.15011) ([Code](https://github.com/allenai/scitldr), [Demo](https://scitldr.apps.allenai.org/))<br />Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld | gen-ext/abs                                          |

<br>

#### References

- [neulab/Text-Summarization-Papers](https://github.com/neulab/Text-Summarization-Papers)
  - [10 must-read papers for neural **extractive** summarization](http://pfliu.com/pl-summarization/summ_paper_gen-ext.html)
  - [10 must-read papers for neural **abstractive** summarization](http://pfliu.com/pl-summarization/summ_paper_gen-abs.html)
- [icoxfog417/awesome-text-summarization](https://github.com/icoxfog417/awesome-text-summarization)
- [KaiyuanGao/awesome-deeplearning-nlp-papers](https://github.com/KaiyuanGao/awesome-deeplearning-nlp-papers)
- [mathsyouth/awesome-text-summarization](https://github.com/mathsyouth/awesome-text-summarization)

